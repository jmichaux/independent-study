{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "#### Returns\n",
    "We start be defining returns: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +  \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$\n",
    "\n",
    "#### Value functions\n",
    "* _state-value function_:  $v_{\\pi}(s) = \\mathbb{E}_\\pi[G_t | S\\_t = s]$\n",
    "* _action-value function_: $q_{\\pi}(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t =a]$\n",
    "\n",
    "We will derive a more useful form for these value function equations below. But first, let's re-prove the well known [Law of Iterated Expectations](https://en.wikipedia.org/wiki/Law_of_total_expectation) using our notation for the expected return $G_{t+1}$.\n",
    "\n",
    "#### Law of iterated expectations\n",
    "***Theorem***\n",
    "\n",
    "$\\mathbb{E}_{\\pi}[\\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1} = s', S_t = s]|S_t = s] = \\mathbb{E}_{\\pi}[G_{t+1} | S_t = s]$ \n",
    "\n",
    "\n",
    "_Proof_: To keep the notation clean and easy to read we'll drop the subscripts, and denote the random variables $s=S_t$, $g'=G_{t+1}$, $s'=S_{t+1}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}, S_t = s] \n",
    "&= \\mathbb{E}_{\\pi}[g' | s', s] \\\\\\\\\n",
    "&= \\sum_{g'}g'p(g' | s', s)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Conditioning on $S_t = s$ and taking the expectation of the above expression we get:\n",
    "\n",
    "$$\n",
    "\\begin {align}\n",
    "\\mathbb{E}_{\\pi}[\\mathbb{E}_\\pi[G_{t+1} | S_{t+1}, S_t = s] | S_t ]\n",
    "&= \\mathbb{E}_{\\pi}[\\mathbb{E}_\\pi[g' | s', s] |s ] \\\\\n",
    "&= \\sum_{s'} \\sum_{g'}g' p(g' | s', s)  p(s' | s)  \\\\\n",
    "&= \\sum_{s'} \\sum_{g'} \\frac{g' p(g' | s', s)  p(s' | s) p(s)}{p(s)}  \\\\\n",
    "&= \\sum_{s'} \\sum_{g'} \\frac{g' p(g' | s', s)  p(s', s)}{p(s)}  \\\\\n",
    "&= \\sum_{s'} \\sum_{g'} \\frac{g' p(g', s', s)}{p(s)}  \\\\\n",
    "&= \\sum_{s'} \\sum_{g'} g' p(g', s' | s)  \\\\\n",
    "&= \\sum_{g'} \\sum_{s'} g' p(g', s' | s)  \\\\\n",
    "&= \\sum_{g'} g' p(g'| s)  \\\\\n",
    "&= \\mathbb{E}_\\pi[g' | s] \\\\\n",
    "&= \\mathbb{E}_\\pi[G_{t+1} | S_{t}].\n",
    "\\end {align}\n",
    "$$\n",
    "\n",
    "\n",
    "## Bellman's Equations\n",
    "Using the law of iterated expectation, we can expand the state-value function $v_{\\pi}(s)$ as follows:\n",
    "$$\n",
    "\\begin {align}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_\\pi[G_t | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +  \\cdots | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{(t+1)+k+1} | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s', S_t=s] | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \\\\\n",
    "&= \\sum_a\\pi(a|s) \\sum_r p(r | s,a)r + \\gamma \\sum_a\\pi(a|s) \\sum_{s'} p(s' | s,a) v_{\\pi} (s') \\\\\n",
    "&= \\sum_a\\pi(a|s) \\sum_r \\sum_{s'} p(s', r | s,a)r + \\gamma \\sum_a\\pi(a|s) \\sum_{s'} \\sum_r p(s', r | s,a) v_{\\pi} (s') \\\\\n",
    "&= \\sum_a\\pi(a|s) \\sum_{s'} \\sum_r p(s', r | s,a)r + \\gamma \\sum_a\\pi(a|s) \\sum_{s'} \\sum_r p(s', r | s,a) v_{\\pi} (s') \\\\\n",
    "&= \\sum_a\\pi(a|s) \\sum_{s'} \\sum_r p(s', r | s,a)[r + \\gamma v_{\\pi} (s')] \\\\\n",
    "\\end {align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Similarly we can write the action-value function $q_{\\pi}(s,a)$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin {align}\n",
    "q_{\\pi}(s, a) &= \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +  \\cdots | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{(t+1)+k+1} | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s', S_t=s, A_t = a] | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\mathbb{E}_{\\pi}[G_{t+1} | S_{t+1}=s'] | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s, A_t = a] \\\\\n",
    "&= \\sum_{s'} \\sum_r p(s', r | s,a)[r + \\gamma v_{\\pi} (s')] \\\\\n",
    "\\end {align}\n",
    "$$\n",
    "\n",
    "We can now combine the expressions for $v_{\\pi}(s)$ and $q_{\\pi}(s, a)$ to get a new expression for $v_{\\pi}(s)$:\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_a\\pi(a|s) q_{\\pi}(s, a) $$.\n",
    "\n",
    "\n",
    "\n",
    "## Existence and Uniqueness of Solution to Bellman's Equation\n",
    "\n",
    "\n",
    "## Summary\n",
    "In the next section, we're going to show how these recursive Bellman equations can be used to derive algorithms for evaluting policies, and for estimating the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
