{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "\n",
    "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    This is a simple implementation of the Gridworld Cliff\n",
    "    reinforcement learning task.\n",
    "    Adapted from Example 6.6 (page 145) from Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto:\n",
    "    http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\n",
    "    \n",
    "    With inspiration from:\n",
    "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
    "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
    "        [3, 0] as the start at bottom-left\n",
    "        [3, 11] as the goal at bottom-right\n",
    "        [3, 1..10] as the cliff at bottom-center\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward \n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.shape = (4, 12)\n",
    "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
    "\n",
    "        nS = np.prod(self.shape)\n",
    "        nA = 4\n",
    "\n",
    "        # Cliff Location\n",
    "        self._cliff = np.zeros(self.shape, dtype=np.bool)\n",
    "        self._cliff[3, 1:-1] = True\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        P = {}\n",
    "        for s in range(nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(nA)}\n",
    "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "        # Calculate initial state distribution\n",
    "        # We always start in state (3, 0)\n",
    "        isd = np.zeros(nS)\n",
    "        isd[self.start_state_index] = 1.0\n",
    "\n",
    "        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def _limit_coordinates(self, coord):\n",
    "        \"\"\"\n",
    "        Prevent the agent from falling out of the grid world\n",
    "        :param coord: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        \"\"\"\n",
    "        Determine the outcome for an action. Transition Prob is always 1.0. \n",
    "        :param current: Current position on the grid as (row, col) \n",
    "        :param delta: Change in position for transition\n",
    "        :return: (1.0, new_state, reward, done)\n",
    "        \"\"\"\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, -100, False)]\n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_done = tuple(new_position) == terminal_state\n",
    "        return [(1.0, new_state, -1, is_done)]\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            # Print terminal state\n",
    "            elif position == (3, 11):\n",
    "                output = \" T \"\n",
    "            elif self._cliff[position]:\n",
    "                output = \" C \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += '\\n'\n",
    "\n",
    "            outfile.write(output)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(1.0, 0, -1, False)],\n",
      "     1: [(1.0, 1, -1, False)],\n",
      "     2: [(1.0, 12, -1, False)],\n",
      "     3: [(1.0, 0, -1, False)]},\n",
      " 1: {0: [(1.0, 1, -1, False)],\n",
      "     1: [(1.0, 2, -1, False)],\n",
      "     2: [(1.0, 13, -1, False)],\n",
      "     3: [(1.0, 0, -1, False)]},\n",
      " 2: {0: [(1.0, 2, -1, False)],\n",
      "     1: [(1.0, 3, -1, False)],\n",
      "     2: [(1.0, 14, -1, False)],\n",
      "     3: [(1.0, 1, -1, False)]},\n",
      " 3: {0: [(1.0, 3, -1, False)],\n",
      "     1: [(1.0, 4, -1, False)],\n",
      "     2: [(1.0, 15, -1, False)],\n",
      "     3: [(1.0, 2, -1, False)]},\n",
      " 4: {0: [(1.0, 4, -1, False)],\n",
      "     1: [(1.0, 5, -1, False)],\n",
      "     2: [(1.0, 16, -1, False)],\n",
      "     3: [(1.0, 3, -1, False)]},\n",
      " 5: {0: [(1.0, 5, -1, False)],\n",
      "     1: [(1.0, 6, -1, False)],\n",
      "     2: [(1.0, 17, -1, False)],\n",
      "     3: [(1.0, 4, -1, False)]},\n",
      " 6: {0: [(1.0, 6, -1, False)],\n",
      "     1: [(1.0, 7, -1, False)],\n",
      "     2: [(1.0, 18, -1, False)],\n",
      "     3: [(1.0, 5, -1, False)]},\n",
      " 7: {0: [(1.0, 7, -1, False)],\n",
      "     1: [(1.0, 8, -1, False)],\n",
      "     2: [(1.0, 19, -1, False)],\n",
      "     3: [(1.0, 6, -1, False)]},\n",
      " 8: {0: [(1.0, 8, -1, False)],\n",
      "     1: [(1.0, 9, -1, False)],\n",
      "     2: [(1.0, 20, -1, False)],\n",
      "     3: [(1.0, 7, -1, False)]},\n",
      " 9: {0: [(1.0, 9, -1, False)],\n",
      "     1: [(1.0, 10, -1, False)],\n",
      "     2: [(1.0, 21, -1, False)],\n",
      "     3: [(1.0, 8, -1, False)]},\n",
      " 10: {0: [(1.0, 10, -1, False)],\n",
      "      1: [(1.0, 11, -1, False)],\n",
      "      2: [(1.0, 22, -1, False)],\n",
      "      3: [(1.0, 9, -1, False)]},\n",
      " 11: {0: [(1.0, 11, -1, False)],\n",
      "      1: [(1.0, 11, -1, False)],\n",
      "      2: [(1.0, 23, -1, False)],\n",
      "      3: [(1.0, 10, -1, False)]},\n",
      " 12: {0: [(1.0, 0, -1, False)],\n",
      "      1: [(1.0, 13, -1, False)],\n",
      "      2: [(1.0, 24, -1, False)],\n",
      "      3: [(1.0, 12, -1, False)]},\n",
      " 13: {0: [(1.0, 1, -1, False)],\n",
      "      1: [(1.0, 14, -1, False)],\n",
      "      2: [(1.0, 25, -1, False)],\n",
      "      3: [(1.0, 12, -1, False)]},\n",
      " 14: {0: [(1.0, 2, -1, False)],\n",
      "      1: [(1.0, 15, -1, False)],\n",
      "      2: [(1.0, 26, -1, False)],\n",
      "      3: [(1.0, 13, -1, False)]},\n",
      " 15: {0: [(1.0, 3, -1, False)],\n",
      "      1: [(1.0, 16, -1, False)],\n",
      "      2: [(1.0, 27, -1, False)],\n",
      "      3: [(1.0, 14, -1, False)]},\n",
      " 16: {0: [(1.0, 4, -1, False)],\n",
      "      1: [(1.0, 17, -1, False)],\n",
      "      2: [(1.0, 28, -1, False)],\n",
      "      3: [(1.0, 15, -1, False)]},\n",
      " 17: {0: [(1.0, 5, -1, False)],\n",
      "      1: [(1.0, 18, -1, False)],\n",
      "      2: [(1.0, 29, -1, False)],\n",
      "      3: [(1.0, 16, -1, False)]},\n",
      " 18: {0: [(1.0, 6, -1, False)],\n",
      "      1: [(1.0, 19, -1, False)],\n",
      "      2: [(1.0, 30, -1, False)],\n",
      "      3: [(1.0, 17, -1, False)]},\n",
      " 19: {0: [(1.0, 7, -1, False)],\n",
      "      1: [(1.0, 20, -1, False)],\n",
      "      2: [(1.0, 31, -1, False)],\n",
      "      3: [(1.0, 18, -1, False)]},\n",
      " 20: {0: [(1.0, 8, -1, False)],\n",
      "      1: [(1.0, 21, -1, False)],\n",
      "      2: [(1.0, 32, -1, False)],\n",
      "      3: [(1.0, 19, -1, False)]},\n",
      " 21: {0: [(1.0, 9, -1, False)],\n",
      "      1: [(1.0, 22, -1, False)],\n",
      "      2: [(1.0, 33, -1, False)],\n",
      "      3: [(1.0, 20, -1, False)]},\n",
      " 22: {0: [(1.0, 10, -1, False)],\n",
      "      1: [(1.0, 23, -1, False)],\n",
      "      2: [(1.0, 34, -1, False)],\n",
      "      3: [(1.0, 21, -1, False)]},\n",
      " 23: {0: [(1.0, 11, -1, False)],\n",
      "      1: [(1.0, 23, -1, False)],\n",
      "      2: [(1.0, 35, -1, False)],\n",
      "      3: [(1.0, 22, -1, False)]},\n",
      " 24: {0: [(1.0, 12, -1, False)],\n",
      "      1: [(1.0, 25, -1, False)],\n",
      "      2: [(1.0, 36, -1, False)],\n",
      "      3: [(1.0, 24, -1, False)]},\n",
      " 25: {0: [(1.0, 13, -1, False)],\n",
      "      1: [(1.0, 26, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 24, -1, False)]},\n",
      " 26: {0: [(1.0, 14, -1, False)],\n",
      "      1: [(1.0, 27, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 25, -1, False)]},\n",
      " 27: {0: [(1.0, 15, -1, False)],\n",
      "      1: [(1.0, 28, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 26, -1, False)]},\n",
      " 28: {0: [(1.0, 16, -1, False)],\n",
      "      1: [(1.0, 29, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 27, -1, False)]},\n",
      " 29: {0: [(1.0, 17, -1, False)],\n",
      "      1: [(1.0, 30, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 28, -1, False)]},\n",
      " 30: {0: [(1.0, 18, -1, False)],\n",
      "      1: [(1.0, 31, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 29, -1, False)]},\n",
      " 31: {0: [(1.0, 19, -1, False)],\n",
      "      1: [(1.0, 32, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 30, -1, False)]},\n",
      " 32: {0: [(1.0, 20, -1, False)],\n",
      "      1: [(1.0, 33, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 31, -1, False)]},\n",
      " 33: {0: [(1.0, 21, -1, False)],\n",
      "      1: [(1.0, 34, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 32, -1, False)]},\n",
      " 34: {0: [(1.0, 22, -1, False)],\n",
      "      1: [(1.0, 35, -1, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 33, -1, False)]},\n",
      " 35: {0: [(1.0, 23, -1, False)],\n",
      "      1: [(1.0, 35, -1, False)],\n",
      "      2: [(1.0, 47, -1, True)],\n",
      "      3: [(1.0, 34, -1, False)]},\n",
      " 36: {0: [(1.0, 24, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -1, False)],\n",
      "      3: [(1.0, 36, -1, False)]},\n",
      " 37: {0: [(1.0, 25, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -1, False)]},\n",
      " 38: {0: [(1.0, 26, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 39: {0: [(1.0, 27, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 40: {0: [(1.0, 28, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 41: {0: [(1.0, 29, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 42: {0: [(1.0, 30, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 43: {0: [(1.0, 31, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 44: {0: [(1.0, 32, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 45: {0: [(1.0, 33, -1, False)],\n",
      "      1: [(1.0, 36, -100, False)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 46: {0: [(1.0, 34, -1, False)],\n",
      "      1: [(1.0, 47, -1, True)],\n",
      "      2: [(1.0, 36, -100, False)],\n",
      "      3: [(1.0, 36, -100, False)]},\n",
      " 47: {0: [(1.0, 35, -1, False)],\n",
      "      1: [(1.0, 47, -1, True)],\n",
      "      2: [(1.0, 47, -1, True)],\n",
      "      3: [(1.0, 36, -100, False)]}}\n"
     ]
    }
   ],
   "source": [
    "pprint(env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "# actions\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"3x4\": [\n",
    "        \"EEEG\",\n",
    "        \"EXEH\",\n",
    "        \"SEEE\",\n",
    "    ],\n",
    "    \"4x4\": [\n",
    "        \"GEEE\",\n",
    "        \"EEEE\",\n",
    "        \"EEEE\",\n",
    "        \"EEEG\",\n",
    "    ],\n",
    "    \"5x4\": [\n",
    "        \"GEEE\",\n",
    "        \"EEEE\",\n",
    "        \"EEEE\",\n",
    "        \"EEEG\",\n",
    "        \"XEXX\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "class ClassicGridEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Example Gridworld:\n",
    "\n",
    "        EEEG\n",
    "        EXEH\n",
    "        SEEE\n",
    "\n",
    "    S : starting cell\n",
    "    E : empty cell\n",
    "    H : hole\n",
    "    G : goal\n",
    "    X : obstacle\n",
    "\n",
    "    The episode ends when the agent reaches a terminal state (goal or pit). The agent\n",
    "    receives a reward when transitioning in the environment. See below for details.\n",
    "\n",
    "    Rewards r(s, a, s'):\n",
    "        3x4:\n",
    "            r(goal,any action, s') = 1\n",
    "            r(pit, any action, s') = -1,\n",
    "            r(nonterminal state, any action, s') =  -0.1\n",
    "\n",
    "        4x4 or 5x4:\n",
    "            r(nonterminal state, any action, s') =  -1\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "\n",
    "    def __init__(self, desc=None, map_name=None, is_noisy=False, nrew=-0.1):\n",
    "        if desc is None and map_name is None:\n",
    "            raise ValueError('Must provide either desc or map_name')\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc, dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (-1, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        #initial state distribution\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        # Empty dict of dict of lists for environment dynamics\n",
    "        P = {s: {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            # left\n",
    "            if a == 0:\n",
    "                col = max(col-1, 0)\n",
    "            # down\n",
    "            if a == 1:\n",
    "                row = min(row+1, nrow-1)\n",
    "            # right\n",
    "            if a == 2:\n",
    "                col = min(col+1, ncol-1)\n",
    "            if a == 3:\n",
    "                row = max(row-1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        # create state-transition probabilities\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(nA):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter == b'G':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    elif letter == b'H':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    elif letter == b'X':\n",
    "                        li.append((0.0, s, 0, False))\n",
    "                    else:\n",
    "                        if is_noisy:\n",
    "                            for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                                newrow, newcol = inc(row, col, b)\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                                if newletter == b'X':\n",
    "                                    newrow, newcol = row, col\n",
    "                                    newletter = desc[newrow, newcol]\n",
    "                                newstate = to_s(newrow, newcol)\n",
    "                                done = bytes(newletter) in b'GH'\n",
    "                                rew = nrew\n",
    "#                                 if done:\n",
    "#                                     rew = float(newletter == b'G') - float(newletter == b'H')\n",
    "#                                 else:\n",
    "#                                     rew = nrew\n",
    "                                # rew = float(newletter == b'G') - float(newletter == b'H') + nrew * (float(newletter != b'G') * float(newletter != b'H'))\n",
    "                                # rew = -0.02\n",
    "                                # rew = float(newletter == b'G') - float(newletter == b'H') - 0.02\n",
    "                                #\n",
    "                                # if map_name == \"3x4\":\n",
    "                                #     rew = float(newletter == b'G') - float(newletter == b'H') + nrew * (float(newletter != b'G') * float(newletter != b'H'))\n",
    "                                #     # rew = nrew\n",
    "                                # else:\n",
    "                                #     rew = float(newletter == b'G') - 1\n",
    "                                li.append((0.8 if b == a else 0.1, newstate, rew, done))\n",
    "                        else:\n",
    "                            newrow, newcol = inc(row, col, a)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            if newletter == b'X':\n",
    "                                newrow, newcol = row, col\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            if map_name == \"3x4\":\n",
    "                                rew = float(newletter == b'G') - float(newletter == b'H') + nrew * (float(newletter != b'G') * float(newletter != b'H'))\n",
    "                            else:\n",
    "                                rew = float(newletter == b'G') - 1\n",
    "                            li.append((1.0, newstate, rew, done))\n",
    "\n",
    "        super(ClassicGridEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            return outfile\n",
    "\n",
    "\n",
    "class ClassicGridEnv3x4(ClassicGridEnv):\n",
    "    def __init__(self, nrew):\n",
    "        super(ClassicGridEnv3x4, self).__init__(map_name=\"3x4\", is_noisy=True, nrew=nrew)\n",
    "\n",
    "class ClassicGridEnv4x4(ClassicGridEnv):\n",
    "    def __init__(self):\n",
    "        super(ClassicGridEnv4x4, self).__init__(map_name=\"4x4\", is_noisy=False)\n",
    "\n",
    "class ClassicGridEnv5x4Static(ClassicGridEnv):\n",
    "    \"\"\"\n",
    "    Do not allow agent to move from cell 13 to cell 15 (17 in array).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ClassicGridEnv5x4Static, self).__init__(map_name=\"5x4\", is_noisy=False)\n",
    "        self.P[13][1] =  [(1.0, 13, -1.0, False)]\n",
    "        # State 15 is state 17\n",
    "        self.P[17] = {\n",
    "        0: [(1.0, 12, -1.0, False)],\n",
    "        1: [(1.0, 17, -1.0, False)],\n",
    "        2: [(1.0, 14, -1.0, False)],\n",
    "        3: [(1.0, 13, -1.0, False)]\n",
    "        }\n",
    "\n",
    "class ClassicGridEnv5x4Dynamic(ClassicGridEnv):\n",
    "    \"\"\"\n",
    "    Allow agent to move from cell 13 to cell 15 (17 in array).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ClassicGridEnv5x4Dynamic, self).__init__(map_name=\"5x4\", is_noisy=False)\n",
    "        self.P[17] = {\n",
    "        0: [(1.0, 12, -1.0, False)],\n",
    "        1: [(1.0, 17, -1.0, False)],\n",
    "        2: [(1.0, 14, -1.0, False)],\n",
    "        3: [(1.0, 13, -1.0, False)]\n",
    "        }\n",
    "\n",
    "\n",
    "# if __name__ =='__main__':\n",
    "#     import pdb\n",
    "#     from copy import deepcopy\n",
    "#     from collections import defaultdict\n",
    "#     from pprint import pprint\n",
    "#     import sys\n",
    "#     if \"../\" not in sys.path:\n",
    "#         sys.path.append(\"../\")\n",
    "\n",
    "#     import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#     env = ClassicGridEnv3x4(nrew=-0.04)\n",
    "#     optimal_3x4_policy = np.array([\n",
    "#         [0.0, 0.0, 1.0, 0.0],\n",
    "#         [0.0, 0.0, 1.0, 0.0],\n",
    "#         [0.0, 0.0, 1.0, 0.0],\n",
    "#         [0.0, 0.0, 1.0, 0.0],\n",
    "\n",
    "#         [0.0, 0.0, 0.0, 1.0],\n",
    "#         [0.0, 0.0, 0.0, 0.0],\n",
    "#         [0.0, 0.0, 0.0, 1.0],\n",
    "#         [0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "#         [0.0, 0.0, 0.0, 1.0],\n",
    "#         [1.0, 0.0, 0.0, 0.0],\n",
    "#         [0.0, 0.0, 1.0, 0.0],\n",
    "#         [0.0, 0.0, 0.0, 1.0],\n",
    "#     ])\n",
    "    \n",
    "#     print(policy_evaluation(optimal_3x4_policy, env, discount_factor=1.0).reshape((3,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 2\n",
      "{0: [(0.1, 2, -0.04, False), (0.8, 1, -0.04, False), (0.1, 6, -0.04, False)],\n",
      " 1: [(0.1, 1, -0.04, False), (0.8, 6, -0.04, False), (0.1, 3, -0.04, True)],\n",
      " 2: [(0.1, 6, -0.04, False), (0.8, 3, -0.04, True), (0.1, 2, -0.04, False)],\n",
      " 3: [(0.1, 3, -0.04, True), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)]}\n",
      "\n",
      "State 3: Goal\n",
      "{0: [(1.0, 3, 0, True)],\n",
      " 1: [(1.0, 3, 0, True)],\n",
      " 2: [(1.0, 3, 0, True)],\n",
      " 3: [(1.0, 3, 0, True)]}\n",
      "\n",
      "{0: [(0.1, 2, -0.04, False), (0.8, 6, -0.04, False), (0.1, 10, -0.04, False)],\n",
      " 1: [(0.1, 6, -0.04, False), (0.8, 10, -0.04, False), (0.1, 7, -0.04, True)],\n",
      " 2: [(0.1, 10, -0.04, False), (0.8, 7, -0.04, True), (0.1, 2, -0.04, False)],\n",
      " 3: [(0.1, 7, -0.04, True), (0.8, 2, -0.04, False), (0.1, 6, -0.04, False)]}\n",
      "\n",
      "State 7: Hole\n",
      "{0: [(1.0, 7, 0, True)],\n",
      " 1: [(1.0, 7, 0, True)],\n",
      " 2: [(1.0, 7, 0, True)],\n",
      " 3: [(1.0, 7, 0, True)]}\n",
      "\n",
      "State 11\n",
      "{0: [(0.1, 7, -0.04, True), (0.8, 10, -0.04, False), (0.1, 11, -0.04, False)],\n",
      " 1: [(0.1, 10, -0.04, False), (0.8, 11, -0.04, False), (0.1, 11, -0.04, False)],\n",
      " 2: [(0.1, 11, -0.04, False), (0.8, 11, -0.04, False), (0.1, 7, -0.04, True)],\n",
      " 3: [(0.1, 11, -0.04, False), (0.8, 7, -0.04, True), (0.1, 10, -0.04, False)]}\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    V[3] = 1\n",
    "    V[7] = -1\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            if s==3 or s==7:\n",
    "                continue\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "env2 = ClassicGridEnv3x4(nrew=-0.04)\n",
    "optimal_policy = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "])\n",
    "print('State 2')\n",
    "pprint(env2.P[2])\n",
    "print()\n",
    "print('State 3: Goal')\n",
    "pprint(env2.P[3])\n",
    "print()\n",
    "pprint(env2.P[6])\n",
    "print()\n",
    "print('State 7: Hole')\n",
    "pprint(env2.P[7])\n",
    "print()\n",
    "print('State 11')\n",
    "pprint(env2.P[11])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81155822  0.86780822  0.91780822  1.        ]\n",
      " [ 0.76155822  0.          0.66027397 -1.        ]\n",
      " [ 0.70530822  0.65530822 -0.52098982 -0.61865674]]\n"
     ]
    }
   ],
   "source": [
    "print(policy_evaluation(optimal_policy, env2).reshape((3,4))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(0.1, 6, -0.04, False), (0.8, 9, -0.04, False), (0.1, 10, -0.04, False)],\n",
      " 1: [(0.1, 9, -0.04, False), (0.8, 10, -0.04, False), (0.1, 11, -0.04, False)],\n",
      " 2: [(0.1, 10, -0.04, False), (0.8, 11, -0.04, False), (0.1, 6, -0.04, False)],\n",
      " 3: [(0.1, 11, -0.04, False), (0.8, 6, -0.04, False), (0.1, 9, -0.04, False)]}\n"
     ]
    }
   ],
   "source": [
    "pprint(env2.P[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(0.1, 7, -0.04, True), (0.8, 10, -0.04, False), (0.1, 11, -0.04, False)],\n",
      " 1: [(0.1, 10, -0.04, False), (0.8, 11, -0.04, False), (0.1, 11, -0.04, False)],\n",
      " 2: [(0.1, 11, -0.04, False), (0.8, 11, -0.04, False), (0.1, 7, -0.04, True)],\n",
      " 3: [(0.1, 11, -0.04, False), (0.8, 7, -0.04, True), (0.1, 10, -0.04, False)]}\n"
     ]
    }
   ],
   "source": [
    "pprint(env2.P[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
