{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "from envs.classic_gridworld import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=1e-5):\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS-1):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])               \n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Norvig and Russel Chapters 17 and 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81155813  0.86780822  0.91780822  1.        ]\n",
      " [ 0.76155803  0.          0.66027397 -1.        ]\n",
      " [ 0.70530727  0.65530642  0.61141351  0.38792265]]\n"
     ]
    }
   ],
   "source": [
    "env = ClassicGridEnv3x4(nrew=-0.04)\n",
    "policy = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "])\n",
    "\n",
    "print(policy_evaluation(policy, env)[:-1].reshape((3,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: CS229 Lecture 16 (32:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52265345  0.73215214  0.76664901  1.        ]\n",
      " [-0.89853063  0.         -0.82069941 -1.        ]\n",
      " [-0.88462558 -0.86880462 -0.85452187 -0.99511395]]\n"
     ]
    }
   ],
   "source": [
    "env = ClassicGridEnv3x4(nrew=-0.02)\n",
    "policy = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "])\n",
    "\n",
    "print(policy_evaluation(policy, env, 0.99)[:-1].reshape((3,4))) \n",
    "V = policy_evaluation(policy, env, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: CS229 Lecture 16 (32:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original policy \n",
      " [[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]] \n",
      "\n",
      "State Values \n",
      " [[ 0.52265345  0.73215214  0.76664901  1.        ]\n",
      " [-0.89853063  0.         -0.82069941 -1.        ]\n",
      " [-0.88462558 -0.86880462 -0.85452187 -0.99511395]] \n",
      "\n",
      "New policy\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]] \n",
      "\n",
      "State Values\n",
      "[[ 0.85530046  0.89580324  0.93236641  1.        ]\n",
      " [ 0.8196974   0.          0.68749634 -1.        ]\n",
      " [ 0.58789645  0.59159775  0.6243202   0.41671652]]\n",
      "\n",
      "Optimal policy\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "State Values\n",
      "[[ 0.53806204  0.7494399   0.7744399   1.        ]\n",
      " [-0.95297074  0.         -0.83004094 -1.        ]\n",
      " [-0.92797372 -0.89984932 -0.87484935 -1.00831659]]\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = ClassicGridEnv3x4(nrew=-0.02)\n",
    "\n",
    "# set policy\n",
    "policy = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "])\n",
    "\n",
    "# Evaluate policy\n",
    "discount_factor = 0.99\n",
    "V = policy_evaluation(policy, env, discount_factor)\n",
    "print('Original policy \\n', policy, '\\n')\n",
    "# print(policy, '\\n')\n",
    "print('State Values \\n', V[:-1].reshape((3,4)), '\\n')\n",
    "# Improve policy\n",
    "new_policy = np.zeros((env.nS, env.nA))\n",
    "for state in env.P:\n",
    "    action_returns = np.zeros(len(env.P[state]))\n",
    "    for i, action in enumerate(env.P[state]):\n",
    "        a = 0.0\n",
    "        for prob, next_state, reward, done in env.P[state][action]:\n",
    "            a += prob * (reward + discount_factor * V[next_state])\n",
    "        action_returns[i] = a\n",
    "    new_action = int(np.argmax(action_returns))\n",
    "    new_policy[state][new_action] = 1.0\n",
    "\n",
    "# Evaluate new policy\n",
    "new_V = policy_evaluation(new_policy, env, discount_factor)\n",
    "\n",
    "print('New policy')\n",
    "print(new_policy, '\\n')\n",
    "print('State Values')\n",
    "print(new_V[:-1].reshape((3,4))) \n",
    "print()\n",
    "\n",
    "optimal_policy = np.array([\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "])\n",
    "print('Optimal policy')\n",
    "print(optimal_policy)\n",
    "optimal_V = policy_evaluation(policy, env)\n",
    "print()\n",
    "print('State Values')\n",
    "print(optimal_V[:-1].reshape((3,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, discount_factor):\n",
    "    new_policy = np.zeros((env.nS, env.nA))\n",
    "    for state in env.P:\n",
    "        action_returns = np.zeros(len(env.P[state]))\n",
    "        for i, action in enumerate(env.P[state]):\n",
    "            a = 0.0\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                a += prob * (reward + discount_factor * V[next_state])\n",
    "            action_returns[i] = a\n",
    "        new_action = int(np.argmax(action_returns))\n",
    "        new_policy[state][new_action] = 1.0   \n",
    "    return new_policy\n",
    "\n",
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=1e-5):\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS-1):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])               \n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "def init_random_policy(evn):\n",
    "    policy = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS):\n",
    "        policy[state][random.randrange(env.nA)] = 1.0\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, discount_factor=1.0, theta=1e-5):\n",
    "    # Initiate random policy\n",
    "    policy = init_random_policy(env)\n",
    "    while True:\n",
    "        # Evaluate policy\n",
    "        V = policy_evaluation(policy, env, discount_factor, theta)\n",
    "        \n",
    "        # Improve policy\n",
    "        new_policy = policy_improvement(env, V, discount_factor)\n",
    "        \n",
    "        # Differences between policies\n",
    "        num_differences = np.sum(np.abs(policy - new_policy))\n",
    "        if num_differences == 0:\n",
    "            policy = new_policy\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "opt_policy = policy_iteration(env, 0.99)[:-1]\n",
    "print(np.sum(np.abs(opt_policy[[0,1,2,4,6,8,9,10,11]] - optimal_policy[[0,1,2,4,6,8,9,10,11]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
